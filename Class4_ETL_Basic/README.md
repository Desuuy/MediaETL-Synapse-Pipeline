# Há»‡ thá»‘ng ETL Media Analytics - HÆ°á»›ng dáº«n Step-by-Step

> **ğŸ¯ NgÆ°á»i má»›i báº¯t Ä‘áº§u?** Xem file **[pipeline_setup/SETUP_GUIDE.md](./pipeline_setup/SETUP_GUIDE.md)** Ä‘á»ƒ setup tá»« Ä‘áº§u KHÃ”NG Lá»–I!

## ğŸ“‹ Tá»•ng quan há»‡ thá»‘ng

Há»‡ thá»‘ng ETL hoÃ n chá»‰nh bao gá»“m:
1. **ETL Application** - Pháº§n má»m ETL vá»›i giao diá»‡n interactive
2. **Data Warehouse (DW)** - Database normalized Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u chi tiáº¿t
3. **Data Mart (DM)** - Database denormalized Ä‘á»ƒ tá»‘i Æ°u cho reporting
4. **Real-time Dashboards** - Streamlit vÃ  Grafana dashboard hiá»ƒn thá»‹ dá»¯ liá»‡u real-time

### âš¡ Quick Start - Cháº¡y táº¥t cáº£ trong má»™t lá»‡nh

```bash
# Cháº¡y full ETL pipeline tá»± Ä‘á»™ng (load data + stored procedures + dashboard)
python C:\Users\anhhu\Downloads\Study_DE\Project\Class4_ETL_Basic\Scripts\run_full_etl.py
```

Script nÃ y sáº½ tá»± Ä‘á»™ng:
- âœ… Load data vÃ o Staging
- âœ… Cháº¡y stored procedures (DW â†’ DM)
- âœ… Khá»Ÿi Ä‘á»™ng Streamlit dashboard vá»›i **real-time auto-refresh**

---

## ğŸ”§ Environment Setup

### CÃ i Ä‘áº·t Python packages

```bash
pip install streamlit pandas pyodbc plotly pyspark findspark
```

### CÃ i Ä‘áº·t SQL Server vÃ  JDBC Driver

1. **CÃ i Ä‘áº·t SQL Server** (SQL Server 2019 hoáº·c má»›i hÆ¡n)
2. **Download vÃ  cÃ i Ä‘áº·t** [Microsoft JDBC Driver for SQL Server](https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server)
3. **Copy file `sqljdbc_auth.dll`** vÃ o thÆ° má»¥c Spark jars:
   - Windows: `C:\spark\spark-4.0.0-bin-hadoop3\jars\`

### Cáº¥u hÃ¬nh SQL Server

- Äáº£m báº£o SQL Server Ä‘ang cháº¡y
- Cho phÃ©p SQL Server Authentication hoáº·c sá»­ dá»¥ng Windows Authentication
- Táº¡o thÆ° má»¥c `C:\Data\` Ä‘á»ƒ lÆ°u database files (hoáº·c thay Ä‘á»•i path trong script)

---

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### BÆ°á»›c 1: Táº¡o Database vÃ  Schema

Má»Ÿ **SQL Server Management Studio (SSMS)** vÃ  cháº¡y cÃ¡c script theo thá»© tá»±:

```sql
-- 1. Táº¡o databases
-- Cháº¡y: Database/01_Create_Database.sql

-- 2. Táº¡o Data Warehouse schema (normalized)
-- Cháº¡y: Database/02_Create_DW_Schema.sql

-- 3. Táº¡o Data Mart schema (denormalized)
-- Cháº¡y: Database/03_Create_DM_Schema.sql

-- 4. Populate dimension tables
-- Cháº¡y: Database/04_Populate_DimDate.sql

-- 5. Táº¡o ELT stored procedures (ÄÃƒ ÄÆ¯á»¢C FIX - KHÃ”NG Lá»–I)
-- Cháº¡y: Database/05_ELT_Load_DW.sql â­
--   âœ… ÄÃ£ fix: MERGE Ä‘á»ƒ xá»­ lÃ½ duplicate MacAddress
--   âœ… ÄÃ£ fix: ProcessDate = NULL xá»­ lÃ½ Táº¤T Cáº¢ dá»¯ liá»‡u
-- Cháº¡y: Database/06_ELT_Load_DM.sql â­
--   âœ… ÄÃ£ fix: ProcessDate = NULL xá»­ lÃ½ Táº¤T Cáº¢ dá»¯ liá»‡u
--   âœ… ÄÃ£ fix: KhÃ´ng tham chiáº¿u Ä‘áº¿n Fact_MediaReport

-- 6. Táº¡o SQL Server Agent Jobs (optional)
-- Cháº¡y: Database/07_Create_Scheduler.sql
```

**LÆ°u Ã½:** Náº¿u báº¡n khÃ´ng cÃ³ quyá»n táº¡o SQL Server Agent Jobs, cÃ³ thá»ƒ bá» qua bÆ°á»›c 6 vÃ  cháº¡y stored procedures thá»§ cÃ´ng.

**Xem hÆ°á»›ng dáº«n chi tiáº¿t:** [pipeline_setup/SETUP_GUIDE.md](./pipeline_setup/SETUP_GUIDE.md)

---

### BÆ°á»›c 2: Cháº¡y ETL Application

#### Option 1: Cháº¡y Full ETL Pipeline (Khuyáº¿n nghá»‹)

**Script master tá»± Ä‘á»™ng cháº¡y táº¥t cáº£ cÃ¡c bÆ°á»›c:**
- Load data vÃ o Staging
- Cháº¡y stored procedures (DW vÃ  DM)
- Khá»Ÿi Ä‘á»™ng Streamlit dashboard vá»›i real-time

```bash
# Náº¿u Ä‘ang á»Ÿ thÆ° má»¥c Project:
python Class4_ETL_Basic/run_full_etl.py

# Hoáº·c náº¿u Ä‘ang á»Ÿ trong thÆ° má»¥c Class4_ETL_Basic:
python run_full_etl.py
```

**CÃ¡c bÆ°á»›c trong á»©ng dá»¥ng:**
1. Nháº­p thÃ´ng tin SQL Server vÃ  authentication
2. Nháº­p Ä‘Æ°á»ng dáº«n thÆ° má»¥c chá»©a file JSON
3. Chá»n cháº¿ Ä‘á»™:
   - **Option 1:** Chá»n khoáº£ng ngÃ y (nháº­p YYYYMMDD)
   - **Option 2:** Chá»n táº¥t cáº£ file trong thÆ° má»¥c
4. Chá»n process date (Enter Ä‘á»ƒ process táº¥t cáº£)
5. Script sáº½ tá»± Ä‘á»™ng:
   - Load vÃ o Staging_RawData
   - Cháº¡y `sp_ELT_LoadToDataWarehouse`
   - Cháº¡y `sp_ELT_LoadToDataMart`
6. Chá»n cÃ³ khá»Ÿi Ä‘á»™ng Streamlit Dashboard ngay khÃ´ng

**Streamlit Dashboard sáº½ tá»± Ä‘á»™ng refresh real-time** (máº·c Ä‘á»‹nh 10 giÃ¢y)

#### Option 2: Cháº¡y tá»«ng bÆ°á»›c riÃªng láº»

##### 2.1. Cháº¡y ETL Application (Xuáº¥t CSV)

```bash
python Class4_ETL_Basic/ETL_Application.py
```

**CÃ¡c bÆ°á»›c trong á»©ng dá»¥ng:**
1. Nháº­p Ä‘Æ°á»ng dáº«n thÆ° má»¥c chá»©a file JSON
2. Chá»n cháº¿ Ä‘á»™:
   - **Option 1:** Chá»n khoáº£ng ngÃ y (nháº­p YYYYMMDD)
   - **Option 2:** Chá»n táº¥t cáº£ file trong thÆ° má»¥c
3. Chá»n output:
   - **Option 1:** CSV File
   - **Option 2:** SQL Server Database

##### 2.2. Cháº¡y ETL Load vÃ o Database

```bash
python Class4_ETL_Basic/ETL_LoadToDatabase.py
```

**CÃ¡c bÆ°á»›c:**
1. Nháº­p thÃ´ng tin SQL Server
2. Chá»n authentication (Windows hoáº·c SQL Server)
3. Nháº­p Ä‘Æ°á»ng dáº«n file JSON
4. Chá»n cháº¿ Ä‘á»™ láº¥y file (khoáº£ng ngÃ y hoáº·c táº¥t cáº£)

**Sau khi load xong, cháº¡y stored procedures:**

```sql
-- Load tá»« Staging vÃ o Data Warehouse
USE DW_MediaAnalytics
GO
EXEC sp_ELT_LoadToDataWarehouse @ProcessDate = NULL  -- NULL = process táº¥t cáº£

-- Load tá»« Data Warehouse xuá»‘ng Data Mart
USE DM_MediaAnalytics
GO
EXEC sp_ELT_LoadToDataMart @ProcessDate = NULL
```

#### Option 3: Chá»‰ cháº¡y Dashboard

```bash
cd Class4_ETL_Basic/Dashboard
streamlit run app.py
```

---

### BÆ°á»›c 3: Cáº¥u hÃ¬nh Grafana Dashboard

#### 3.1. CÃ i Ä‘áº·t Grafana

**Windows:**
1. Download: https://grafana.com/grafana/download?platform=windows
2. CÃ i Ä‘áº·t vÃ  cháº¡y
3. Má»Ÿ: http://localhost:3000
4. Login: admin/admin

**Docker:**
```bash
docker run -d -p 3000:3000 --name=grafana grafana/grafana
```

#### 3.2. CÃ i Ä‘áº·t SQL Server Plugin

1. VÃ o **Configuration** â†’ **Plugins**
2. TÃ¬m "Microsoft SQL Server"
3. Click **Install**

#### 3.3. Táº¡o Datasource

1. VÃ o **Configuration** â†’ **Data Sources** â†’ **Add data source**
2. Chá»n **Microsoft SQL Server**
3. Äiá»n thÃ´ng tin:
   - Name: `SQL Server - Media Analytics`
   - Host: `localhost:1433` (hoáº·c `host.docker.internal:1433` náº¿u dÃ¹ng Docker)
   - Database: `DM_MediaAnalytics`
   - Authentication: Windows Authentication hoáº·c SQL Server Authentication
   - Náº¿u dÃ¹ng SQL Server Authentication:
     - Username: `grafana_user`
     - Password: `Grafana@123`
4. Click **Save & Test**

#### 3.4. Import Dashboards

1. VÃ o **Dashboards** â†’ **Import**
2. Upload cÃ¡c file JSON tá»« thÆ° má»¥c `Grafana/`:
   - `dashboard_overview.json` - Overview dashboard
   - `dashboard_contract_analytics.json` - Contract analytics
   - `dashboard_content_trends.json` - Content type trends
3. Chá»n datasource: **SQL Server - Media Analytics**
4. Click **Import**

**CÃ¡c tÃ­nh nÄƒng Dashboard:**
- **Overview Dashboard:** KPI cards, daily trends, content type distribution
- **Contract Analytics:** Top contracts, contract details, filters
- **Content Type Trends:** Trends over time, comparisons, statistics

**Cáº¥u hÃ¬nh Auto Refresh (Real-time):**
- Má»Ÿ dashboard â†’ Click **âš™ï¸** (Settings) â†’ Set Refresh: `10s` hoáº·c `30s`
- Hoáº·c click vÃ o dropdown refresh á»Ÿ gÃ³c trÃªn bÃªn pháº£i â†’ Chá»n `10s`, `30s`, `1m`, `5m`, etc.
- Dashboard sáº½ tá»± Ä‘á»™ng refresh vÃ  hiá»ƒn thá»‹ dá»¯ liá»‡u real-time

**LÆ°u Ã½:** Äá»ƒ Grafana hiá»ƒn thá»‹ real-time tá»‘t nháº¥t:
1. Äáº£m báº£o SQL Server Ä‘ang cháº¡y vÃ  cÃ³ dá»¯ liá»‡u má»›i
2. Set refresh interval phÃ¹ há»£p (khuyáº¿n nghá»‹: 10s-30s cho real-time, 1m-5m cho monitoring thÃ´ng thÆ°á»ng)
3. CÃ¡c panel sáº½ tá»± Ä‘á»™ng cáº­p nháº­t khi cÃ³ dá»¯ liá»‡u má»›i

**Xem hÆ°á»›ng dáº«n chi tiáº¿t:** [pipeline_setup/HUONG_DAN_MO_GRAFANA.md](./pipeline_setup/HUONG_DAN_MO_GRAFANA.md)

---

## ğŸ“Š Kiáº¿n trÃºc Database

### Data Warehouse (Normalized)

**Dimension Tables:**
- `DimDate` - ThÃ´ng tin ngÃ y thÃ¡ng
- `DimContract` - ThÃ´ng tin Contract
- `DimDevice` - ThÃ´ng tin Device
- `DimApp` - ThÃ´ng tin Application
- `DimContentType` - Loáº¡i ná»™i dung

**Fact Tables:**
- `FactViewingSession` - Chi tiáº¿t session (normalized)
- `FactContractSummary` - Tá»•ng há»£p theo Contract (má»™t pháº§n denormalized)

**Staging Table:**
- `Staging_RawData` - Table táº¡m Ä‘á»ƒ load data tá»« Spark

### Data Mart (Denormalized)

**Tables:**
- `DM_ContractAnalytics` - PhÃ¢n tÃ­ch Contract (denormalized)
- `DM_DailySummary` - TÃ³m táº¯t theo ngÃ y
- `DM_ContentTypeTrend` - Xu hÆ°á»›ng Content Type

**Xem chi tiáº¿t:** [Database/README_SQL.md](./Database/README_SQL.md)

---

## ğŸ”„ Quy trÃ¬nh ETL/ELT

### ETL Process (Extract â†’ Transform â†’ Load)

1. **Extract:** Äá»c file JSON tá»« thÆ° má»¥c
2. **Transform:** 
   - Select fields tá»« `_source`
   - Transform AppName thÃ nh ContentType
   - TÃ­nh toÃ¡n statistics
3. **Load:** 
   - **Option 1:** LÆ°u vÃ o CSV
   - **Option 2:** Load vÃ o SQL Server Staging table

### ELT Process (Extract â†’ Load â†’ Transform)

1. **Extract:** Äá»c tá»« Staging_RawData
2. **Load:** Load vÃ o Dimension vÃ  Fact tables
3. **Transform:** 
   - Transform vÃ  aggregate trong SQL
   - Load vÃ o Data Mart (denormalized)

---

## â° Tá»± Ä‘á»™ng hÃ³a vá»›i SQL Server Agent

Náº¿u Ä‘Ã£ táº¡o SQL Server Agent Jobs (bÆ°á»›c 1), há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng cháº¡y:

- **Job 1:** `ETL_LoadToDataWarehouse` - Cháº¡y hÃ ng ngÃ y lÃºc 2:00 AM
- **Job 2:** `ETL_LoadToDataMart` - Cháº¡y hÃ ng ngÃ y lÃºc 3:00 AM

**Cháº¡y thá»§ cÃ´ng náº¿u cáº§n:**

```sql
-- Cháº¡y Job thá»§ cÃ´ng
USE msdb
GO
EXEC sp_start_job @job_name = 'ETL_LoadToDataWarehouse'
EXEC sp_start_job @job_name = 'ETL_LoadToDataMart'
```

---

## ğŸ“ VÃ­ dá»¥ sá»­ dá»¥ng

### VÃ­ dá»¥ 1: ETL má»™t khoáº£ng ngÃ y vÃ  xuáº¥t CSV

```bash
python Class4_ETL_Basic/ETL_Application.py
# Chá»n: 1 (khoáº£ng ngÃ y)
# Nháº­p: 20220401 (start), 20220405 (end)
# Chá»n: 1 (CSV)
# Nháº­p: C:\Output\ETL_Results
```

### VÃ­ dá»¥ 2: ETL táº¥t cáº£ file vÃ  load vÃ o Database

```bash
python Class4_ETL_Basic/ETL_LoadToDatabase.py
# Nháº­p: localhost
# Database: DW_MediaAnalytics
# Auth: 1 (Windows) hoáº·c 2 (SQL Server)
# Path: C:\Users\anhhu\Downloads\Study_DE\Project\Data\log_content
# Mode: 2 (táº¥t cáº£)
```

Sau Ä‘Ã³ cháº¡y stored procedures trong SSMS.

### VÃ­ dá»¥ 3: Cháº¡y Full ETL Pipeline vÃ  Dashboard

```bash
# Cháº¡y full pipeline (tá»± Ä‘á»™ng hÃ³a táº¥t cáº£)
python C:\Users\anhhu\Downloads\Study_DE\Project\Class4_ETL_Basic\Scripts\run_full_etl.py

# Input máº«u:
# SQL Server: localhost
# Data Warehouse: [Enter] (máº·c Ä‘á»‹nh: DW_MediaAnalytics)
# Data Mart: [Enter] (máº·c Ä‘á»‹nh: DM_MediaAnalytics)
# Authentication: 1-Windows/2-SQL Server
# Náº¿u chá»n 2:
#   Username: grafana_user
#   Password: Grafana@123
# Encrypt: [Enter] (máº·c Ä‘á»‹nh: Enable)
# Trust Server Certificate: [Enter] (máº·c Ä‘á»‹nh: Yes)
# ÄÆ°á»ng dáº«n thÆ° má»¥c: C:\Users\anhhu\Downloads\Study_DE\Project\Data\log_content
# Cháº¿ Ä‘á»™: 
#   1: Chá»n khoáº£ng ngÃ y
#      20220401
#      20220403
#   2: All
# Process date: [Enter] (NULL = process táº¥t cáº£)
# Khá»Ÿi Ä‘á»™ng Dashboard: yes
# Port: [Enter] (máº·c Ä‘á»‹nh: 8501)
```

**Dashboard Real-time Features:**
- âœ… Auto-refresh tá»± Ä‘á»™ng (máº·c Ä‘á»‹nh 10 giÃ¢y)
- âœ… Hiá»ƒn thá»‹ thá»i gian cáº­p nháº­t cuá»‘i cÃ¹ng
- âœ… Countdown timer trÆ°á»›c khi refresh
- âœ… Real-time indicator trong sidebar
- âœ… CÃ³ thá»ƒ táº¯t/báº­t auto-refresh vÃ  Ä‘iá»u chá»‰nh interval

Má»Ÿ browser vÃ  cáº¥u hÃ¬nh database connection trong sidebar. Dashboard sáº½ tá»± Ä‘á»™ng refresh Ä‘á»ƒ hiá»ƒn thá»‹ dá»¯ liá»‡u má»›i nháº¥t.

---

## ğŸ› Troubleshooting

### Lá»—i: "No module named 'pyspark'"
```bash
pip install pyspark findspark
```

### Lá»—i: "JDBC Driver not found"
- Download vÃ  cÃ i Ä‘áº·t Microsoft JDBC Driver
- Copy `sqljdbc_auth.dll` vÃ o thÆ° má»¥c Spark jars

### Lá»—i: "Cannot connect to SQL Server"
- Kiá»ƒm tra SQL Server Ä‘ang cháº¡y
- Kiá»ƒm tra firewall settings
- Kiá»ƒm tra authentication method
- Xem: [pipeline_setup/sql_connect_trouble.md](./pipeline_setup/sql_connect_trouble.md)

### Lá»—i: "Table does not exist"
- Cháº¡y láº¡i cÃ¡c script SQL táº¡o schema (bÆ°á»›c 1)
- Xem: [pipeline_setup/SETUP_GUIDE.md](./pipeline_setup/SETUP_GUIDE.md)

### Lá»—i: "Login failed" hoáº·c "Cannot open database"
- Cháº¡y `Database/DB_Fixed/GRANT_PERMISSIONS.sql` Ä‘á»ƒ cáº¥p quyá»n
- Xem: [pipeline_setup/sql_fix_permission.md](./pipeline_setup/sql_fix_permission.md)

### Lá»—i: "Insufficient disk space"
- Cháº¡y `Database/DB_Fixed/Truncate_DM.sql` Ä‘á»ƒ giáº£i phÃ³ng space
- Hoáº·c cháº¡y `Database/DB_Fixed/FIX_DATABASE_SIZE.sql` Ä‘á»ƒ tÄƒng database size

### Lá»—i: "Sá»‘ dÃ²ng Ä‘Ã£ xá»­ lÃ½: 0"
- Cháº¡y `Database/DB_Fixed/DIAGNOSE_DATA_ISSUES.sql` Ä‘á»ƒ cháº©n Ä‘oÃ¡n
- Kiá»ƒm tra dá»¯ liá»‡u trong Staging_RawData

**Xem táº¥t cáº£ lá»—i vÃ  giáº£i phÃ¡p:** [pipeline_setup/Solution.md](./pipeline_setup/Solution.md)

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

### HÆ°á»›ng dáº«n chÃ­nh
- **Setup Guide:** [pipeline_setup/SETUP_GUIDE.md](./pipeline_setup/SETUP_GUIDE.md) â­ - HÆ°á»›ng dáº«n setup tá»« Ä‘áº§u KHÃ”NG Lá»–I
- **Quick Start:** [pipeline_setup/quick_start.md](./pipeline_setup/quick_start.md) - Cháº¡y nhanh trong 5 phÃºt
- **HÆ°á»›ng dáº«n cháº¡y script:** [pipeline_setup/HUONG_DAN_CHAY_SCRIPT.md](./pipeline_setup/HUONG_DAN_CHAY_SCRIPT.md) - HÆ°á»›ng dáº«n cháº¡y `run_full_etl.py`
- **Debug Guide:** [pipeline_setup/DEBUG_GUIDE.md](./pipeline_setup/DEBUG_GUIDE.md) - HÆ°á»›ng dáº«n debug chung

### Grafana
- **HÆ°á»›ng dáº«n Grafana:** [pipeline_setup/HUONG_DAN_MO_GRAFANA.md](./pipeline_setup/HUONG_DAN_MO_GRAFANA.md) - CÃ i Ä‘áº·t, cáº¥u hÃ¬nh, sá»­ dá»¥ng
- **Setup Grafana:** [pipeline_setup/setup_grafana.md](./pipeline_setup/setup_grafana.md) - HÆ°á»›ng dáº«n setup Grafana

### Database & Troubleshooting
- **SQL Files Guide:** [Database/README_SQL.md](./Database/README_SQL.md) - HÆ°á»›ng dáº«n sá»­ dá»¥ng cÃ¡c file SQL
- **Sá»­a lá»—i permission:** [pipeline_setup/sql_fix_permission.md](./pipeline_setup/sql_fix_permission.md) - Sá»­a lá»—i CREATE DATABASE permission denied
- **SQL Connection Troubleshooting:** [pipeline_setup/sql_connect_trouble.md](./pipeline_setup/sql_connect_trouble.md) - Sá»­a lá»—i káº¿t ná»‘i SQL Server
- **Solution:** [pipeline_setup/Solution.md](./pipeline_setup/Solution.md) â­ - Tá»•ng há»£p Táº¤T Cáº¢ lá»—i vÃ  giáº£i phÃ¡p

### Files SQL chÃ­nh (ÄÃƒ ÄÆ¯á»¢C FIX):
- **`Database/05_ELT_Load_DW.sql`** â­ - Stored procedure DW (Ä‘Ã£ fix MERGE vÃ  ProcessDate)
- **`Database/06_ELT_Load_DM.sql`** â­ - Stored procedure DM (Ä‘Ã£ fix ProcessDate vÃ  Fact_MediaReport)

**TÃ i liá»‡u ká»¹ thuáº­t:**
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [SQL Server Documentation](https://docs.microsoft.com/en-us/sql/)
- [Grafana Documentation](https://grafana.com/docs/grafana/latest/)

---

## ğŸ” Grafana Account (Náº¿u sá»­ dá»¥ng SQL Server Authentication)

```
Username: grafana_user
Password: Grafana@123
Database: DM_MediaAnalytics
Auth: SQL Server Authentication
Role: db_datareader (read-only)
```

**LÆ°u Ã½:** Account nÃ y chá»‰ cÃ³ quyá»n Ä‘á»c dá»¯ liá»‡u. Äá»ƒ cháº¡y ETL pipeline, cáº§n account cÃ³ quyá»n EXECUTE stored procedures. Xem: [Database/DB_Fixed/GRANT_PERMISSIONS.sql](./Database/DB_Fixed/GRANT_PERMISSIONS.sql)

---

## âœ… Äáº£m báº£o khÃ´ng lá»—i

Táº¥t cáº£ cÃ¡c file SQL chÃ­nh Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t vÃ  fix:
- âœ… MERGE statement Ä‘á»ƒ xá»­ lÃ½ duplicate
- âœ… ProcessDate = NULL xá»­ lÃ½ Táº¤T Cáº¢ dá»¯ liá»‡u
- âœ… KhÃ´ng cÃ²n reference Ä‘áº¿n Fact_MediaReport
- âœ… Error handling Ä‘áº§y Ä‘á»§
- âœ… Comments rÃµ rÃ ng

**NgÆ°á»i má»›i chá»‰ cáº§n cháº¡y cÃ¡c file SQL theo thá»© tá»± trong [pipeline_setup/SETUP_GUIDE.md](./pipeline_setup/SETUP_GUIDE.md) lÃ  sáº½ khÃ´ng gáº·p lá»—i!**

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
Class4_ETL_Basic/
â”œâ”€â”€ Database/                    # SQL scripts
â”‚   â”œâ”€â”€ 01_Create_Database.sql
â”‚   â”œâ”€â”€ 02_Create_DW_Schema.sql
â”‚   â”œâ”€â”€ 03_Create_DM_Schema.sql
â”‚   â”œâ”€â”€ 04_Populate_DimDate.sql
â”‚   â”œâ”€â”€ 05_ELT_Load_DW.sql      # â­ Stored procedure DW
â”‚   â”œâ”€â”€ 06_ELT_Load_DM.sql      # â­ Stored procedure DM
â”‚   â”œâ”€â”€ 07_Create_Scheduler.sql
â”‚   â”œâ”€â”€ README_SQL.md
â”‚   â””â”€â”€ DB_Fixed/               # Scripts há»— trá»£ vÃ  fix lá»—i
â”œâ”€â”€ Dashboard/                   # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ Grafana/                     # Grafana dashboards
â”‚   â”œâ”€â”€ dashboard_overview.json
â”‚   â”œâ”€â”€ dashboard_contract_analytics.json
â”‚   â”œâ”€â”€ dashboard_content_trends.json
â”‚   â””â”€â”€ datasource_sqlserver.json
â”œâ”€â”€ pipeline_setup/              # TÃ i liá»‡u hÆ°á»›ng dáº«n
â”‚   â”œâ”€â”€ SETUP_GUIDE.md          # â­ HÆ°á»›ng dáº«n setup
â”‚   â”œâ”€â”€ Solution.md             # â­ Tá»•ng há»£p lá»—i vÃ  giáº£i phÃ¡p
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ETL_Application.py           # ETL app xuáº¥t CSV
â”œâ”€â”€ ETL_LoadToDatabase.py       # ETL load vÃ o database
â”œâ”€â”€ run_full_etl.py             # â­ Script master tá»± Ä‘á»™ng
â””â”€â”€ README.md                   # File nÃ y
```

---

**Cáº­p nháº­t láº§n cuá»‘i:** 2026-01-16  
**PhiÃªn báº£n:** 2.0
