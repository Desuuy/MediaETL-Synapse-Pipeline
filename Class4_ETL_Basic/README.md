# H·ªá th·ªëng ETL Media Analytics - H∆∞·ªõng d·∫´n Step-by-Step

## üìã T·ªïng quan h·ªá th·ªëng

H·ªá th·ªëng ETL ho√†n ch·ªânh bao g·ªìm:
1. **ETL Application** - Ph·∫ßn m·ªÅm ETL v·ªõi giao di·ªán interactive
2. **Data Warehouse (DW)** - Database normalized ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu chi ti·∫øt
3. **Data Mart (DM)** - Database denormalized ƒë·ªÉ t·ªëi ∆∞u cho reporting
4. **Real-time Dashboard** - Grafana dashboard hi·ªÉn th·ªã d·ªØ li·ªáu real-time

---

## üöÄ H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng

### B∆∞·ªõc 1: Chu·∫©n b·ªã m√¥i tr∆∞·ªùng

#### 1.1. C√†i ƒë·∫∑t Python v√† c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt

```bash
# C√†i ƒë·∫∑t Python packages
pip install pyspark findspark pyodbc streamlit pandas plotly
```

#### 1.2. C√†i ƒë·∫∑t SQL Server v√† SQL Server JDBC Driver

- C√†i ƒë·∫∑t SQL Server (SQL Server 2019 ho·∫∑c m·ªõi h∆°n)
- Download v√† c√†i ƒë·∫∑t [Microsoft JDBC Driver for SQL Server](https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server)
- Copy file `sqljdbc_auth.dll` v√†o th∆∞ m·ª•c Spark jars:
  - Windows: `C:\spark\spark-4.0.0-bin-hadoop3\jars\`

#### 1.3. C·∫•u h√¨nh SQL Server

- ƒê·∫£m b·∫£o SQL Server ƒëang ch·∫°y
- Cho ph√©p SQL Server Authentication ho·∫∑c s·ª≠ d·ª•ng Windows Authentication
- T·∫°o th∆∞ m·ª•c `C:\Data\` ƒë·ªÉ l∆∞u database files (ho·∫∑c thay ƒë·ªïi path trong script)

---

### B∆∞·ªõc 2: T·∫°o Database v√† Schema

#### 2.1. Ch·∫°y c√°c script SQL theo th·ª© t·ª±

M·ªü **SQL Server Management Studio (SSMS)** v√† ch·∫°y c√°c script theo th·ª© t·ª±:

```sql
-- 1. T·∫°o databases
-- Ch·∫°y: Database/01_Create_Database.sql

-- 2. T·∫°o Data Warehouse schema (normalized)
-- Ch·∫°y: Database/02_Create_DW_Schema.sql

-- 3. T·∫°o Data Mart schema (denormalized)
-- Ch·∫°y: Database/03_Create_DM_Schema.sql

-- 4. Populate dimension tables
-- Ch·∫°y: Database/04_Populate_DimDate.sql

-- 5. T·∫°o ELT stored procedures
-- Ch·∫°y: Database/05_ELT_Load_DW.sql
-- Ch·∫°y: Database/06_ELT_Load_DM.sql

-- 6. T·∫°o SQL Server Agent Jobs (optional)
-- Ch·∫°y: Database/07_Create_Scheduler.sql
```

**L∆∞u √Ω:** N·∫øu b·∫°n kh√¥ng c√≥ quy·ªÅn t·∫°o SQL Server Agent Jobs, c√≥ th·ªÉ b·ªè qua b∆∞·ªõc 6 v√† ch·∫°y stored procedures th·ªß c√¥ng.

---

### B∆∞·ªõc 3: Ch·∫°y ETL Application

#### 3.1. Ch·∫°y ETL Application (Xu·∫•t CSV)

```bash
python Class4_ETL_Basic/ETL_Application.py
```

**C√°c b∆∞·ªõc trong ·ª©ng d·ª•ng:**
1. Nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file JSON
2. Ch·ªçn ch·∫ø ƒë·ªô:
   - **Option 1:** Ch·ªçn kho·∫£ng ng√†y (nh·∫≠p YYYYMMDD)
   - **Option 2:** Ch·ªçn t·∫•t c·∫£ file trong th∆∞ m·ª•c
3. Ch·ªçn output:
   - **Option 1:** CSV File
   - **Option 2:** SQL Server Database

#### 3.2. Ch·∫°y ETL Load v√†o Database

```bash
python Class4_ETL_Basic/ETL_LoadToDatabase.py
```

**C√°c b∆∞·ªõc:**
1. Nh·∫≠p th√¥ng tin SQL Server
2. Ch·ªçn authentication (Windows ho·∫∑c SQL Server)
3. Nh·∫≠p ƒë∆∞·ªùng d·∫´n file JSON
4. Ch·ªçn ch·∫ø ƒë·ªô l·∫•y file (kho·∫£ng ng√†y ho·∫∑c t·∫•t c·∫£)

**Sau khi load xong, ch·∫°y stored procedures:**

```sql
-- Load t·ª´ Staging v√†o Data Warehouse
USE DW_MediaAnalytics
GO
EXEC sp_ELT_LoadToDataWarehouse @ProcessDate = NULL  -- NULL = process t·∫•t c·∫£

-- Load t·ª´ Data Warehouse xu·ªëng Data Mart
USE DM_MediaAnalytics
GO
EXEC sp_ELT_LoadToDataMart @ProcessDate = NULL
```

---

### B∆∞·ªõc 4: C·∫•u h√¨nh Grafana Dashboard

#### 4.1. C√†i ƒë·∫∑t Grafana

**Windows:**
1. Download: https://grafana.com/grafana/download?platform=windows
2. C√†i ƒë·∫∑t v√† ch·∫°y
3. M·ªü: http://localhost:3000
4. Login: admin/admin

**Docker:**
```bash
docker run -d -p 3000:3000 --name=grafana grafana/grafana
```

#### 4.2. C√†i ƒë·∫∑t SQL Server Plugin

1. V√†o **Configuration** ‚Üí **Plugins**
2. T√¨m "Microsoft SQL Server"
3. Click **Install**

#### 4.3. T·∫°o Datasource

1. V√†o **Configuration** ‚Üí **Data Sources** ‚Üí **Add data source**
2. Ch·ªçn **Microsoft SQL Server**
3. ƒêi·ªÅn th√¥ng tin:
   - Name: `SQL Server - Media Analytics`
   - Host: `localhost:1433`
   - Database: `DM_MediaAnalytics`
   - Authentication: Windows Authentication
4. Click **Save & Test**

#### 4.4. Import Dashboards

1. V√†o **Dashboards** ‚Üí **Import**
2. Upload c√°c file JSON t·ª´ th∆∞ m·ª•c `Grafana/`:
   - `dashboard_overview.json` - Overview dashboard
   - `dashboard_contract_analytics.json` - Contract analytics
   - `dashboard_content_trends.json` - Content type trends
3. Ch·ªçn datasource: **SQL Server - Media Analytics**
4. Click **Import**

**C√°c t√≠nh nƒÉng Dashboard:**
- **Overview Dashboard:** KPI cards, daily trends, content type distribution
- **Contract Analytics:** Top contracts, contract details, filters
- **Content Type Trends:** Trends over time, comparisons, statistics

**C·∫•u h√¨nh Auto Refresh:**
- M·ªü dashboard ‚Üí Click **‚öôÔ∏è** ‚Üí Set Refresh: `10s` ho·∫∑c `30s`

**Xem h∆∞·ªõng d·∫´n chi ti·∫øt:** `Grafana/HUONG_DAN_GRAFANA.md`

---

## üìä Ki·∫øn tr√∫c Database

### Data Warehouse (Normalized)

**Dimension Tables:**
- `DimDate` - Th√¥ng tin ng√†y th√°ng
- `DimContract` - Th√¥ng tin Contract
- `DimDevice` - Th√¥ng tin Device
- `DimApp` - Th√¥ng tin Application
- `DimContentType` - Lo·∫°i n·ªôi dung

**Fact Tables:**
- `FactViewingSession` - Chi ti·∫øt session (normalized)
- `FactContractSummary` - T·ªïng h·ª£p theo Contract (m·ªôt ph·∫ßn denormalized)

**Staging Table:**
- `Staging_RawData` - Table t·∫°m ƒë·ªÉ load data t·ª´ Spark

### Data Mart (Denormalized)

**Tables:**
- `DM_ContractAnalytics` - Ph√¢n t√≠ch Contract (denormalized)
- `DM_DailySummary` - T√≥m t·∫Øt theo ng√†y
- `DM_ContentTypeTrend` - Xu h∆∞·ªõng Content Type

---

## üîÑ Quy tr√¨nh ETL/ELT

### ETL Process (Extract ‚Üí Transform ‚Üí Load)

1. **Extract:** ƒê·ªçc file JSON t·ª´ th∆∞ m·ª•c
2. **Transform:** 
   - Select fields t·ª´ `_source`
   - Transform AppName th√†nh ContentType
   - T√≠nh to√°n statistics
3. **Load:** 
   - **Option 1:** L∆∞u v√†o CSV
   - **Option 2:** Load v√†o SQL Server Staging table

### ELT Process (Extract ‚Üí Load ‚Üí Transform)

1. **Extract:** ƒê·ªçc t·ª´ Staging_RawData
2. **Load:** Load v√†o Dimension v√† Fact tables
3. **Transform:** 
   - Transform v√† aggregate trong SQL
   - Load v√†o Data Mart (denormalized)

---

## ‚è∞ T·ª± ƒë·ªông h√≥a v·ªõi SQL Server Agent

N·∫øu ƒë√£ t·∫°o SQL Server Agent Jobs (b∆∞·ªõc 2.1), h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông ch·∫°y:

- **Job 1:** `ETL_LoadToDataWarehouse` - Ch·∫°y h√†ng ng√†y l√∫c 2:00 AM
- **Job 2:** `ETL_LoadToDataMart` - Ch·∫°y h√†ng ng√†y l√∫c 3:00 AM

**Ch·∫°y th·ªß c√¥ng n·∫øu c·∫ßn:**

```sql
-- Ch·∫°y Job th·ªß c√¥ng
USE msdb
GO
EXEC sp_start_job @job_name = 'ETL_LoadToDataWarehouse'
EXEC sp_start_job @job_name = 'ETL_LoadToDataMart'
```

---

## üìù V√≠ d·ª• s·ª≠ d·ª•ng

### V√≠ d·ª• 1: ETL m·ªôt kho·∫£ng ng√†y v√† xu·∫•t CSV

```bash
python ETL_Application.py
# Ch·ªçn: 1 (kho·∫£ng ng√†y)
# Nh·∫≠p: 20220401 (start), 20220405 (end)
# Ch·ªçn: 1 (CSV)
# Nh·∫≠p: C:\Output\ETL_Results
```

### V√≠ d·ª• 2: ETL t·∫•t c·∫£ file v√† load v√†o Database

```bash
python ETL_LoadToDatabase.py
# Nh·∫≠p: localhost
# Database: DW_MediaAnalytics
# Auth: 1 (Windows)
# Path: C:\Data\log_content
# Mode: 2 (t·∫•t c·∫£)
```

Sau ƒë√≥ ch·∫°y stored procedures trong SSMS.

### V√≠ d·ª• 3: Xem Dashboard

```bash
cd Dashboard
streamlit run app.py
```

M·ªü browser v√† c·∫•u h√¨nh database connection trong sidebar.

---

## üêõ Troubleshooting

### L·ªói: "No module named 'pyspark'"
```bash
pip install pyspark findspark
```

### L·ªói: "JDBC Driver not found"
- Download v√† c√†i ƒë·∫∑t Microsoft JDBC Driver
- Copy `sqljdbc_auth.dll` v√†o th∆∞ m·ª•c Spark jars

### L·ªói: "Cannot connect to SQL Server"
- Ki·ªÉm tra SQL Server ƒëang ch·∫°y
- Ki·ªÉm tra firewall settings
- Ki·ªÉm tra authentication method

### L·ªói: "Table does not exist"
- Ch·∫°y l·∫°i c√°c script SQL t·∫°o schema (b∆∞·ªõc 2.1)

---

## üìö T√†i li·ªáu tham kh·∫£o

### H∆∞·ªõng d·∫´n ch√≠nh
- **Quick Start:** `QUICK_START.md` - Ch·∫°y nhanh trong 5 ph√∫t
- **H∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß:** `CHAY_DAY_DU.md` - T·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi, t·ª´ng b∆∞·ªõc chi ti·∫øt
- **H∆∞·ªõng d·∫´n chi ti·∫øt:** `HUONG_DAN_CHI_TIET.md` - Gi·∫£i th√≠ch k·ªπ thu·∫≠t chi ti·∫øt

### Grafana
- **H∆∞·ªõng d·∫´n Grafana:** `Grafana/HUONG_DAN_GRAFANA.md` - C√†i ƒë·∫∑t, c·∫•u h√¨nh, s·ª≠ d·ª•ng (bao g·ªìm Quick Start)
- **Troubleshooting:** `Grafana/TROUBLESHOOTING_SQL_CONNECTION.md` - S·ª≠a l·ªói k·∫øt n·ªëi SQL Server

### Database & Troubleshooting
- **S·ª≠a l·ªói permission:** `Database/FIX_PERMISSION.md` - S·ª≠a l·ªói CREATE DATABASE permission denied
- **Debug guide:** `DEBUG_GUIDE.md` - H∆∞·ªõng d·∫´n debug chung

**T√†i li·ªáu k·ªπ thu·∫≠t:**
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [SQL Server Documentation](https://docs.microsoft.com/en-us/sql/)
- [Grafana Documentation](https://grafana.com/docs/grafana/latest/)

---

## Grafana Account
```
Username : grafana_user
Password : Grafana@123
Database : DM_MediaAnalytics
Auth : SQL Server Authentication
Role : db_datareader (read-only)
```

## SSMS Account 
```
Host : host.docker.internal:1433
Database : DM_MediaAnalytics
User : grafana_user
Password : Grafana@123
Encrypt : Disable (ho·∫∑c Trust Server Certificate)
```